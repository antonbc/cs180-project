{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ba9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8058a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset path\n",
    "training_dataset_path = './train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cebec45",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9595f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=training_dataset_path, \n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92323c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['cordana', 'healthy', 'pestalotiopsis', 'sigatoka']\n",
      "Batch Size: 32\n",
      "Channels: 3\n",
      "Height of Image: 224\n",
      "Width of Image: 224\n",
      "Total Images: 843\n"
     ]
    }
   ],
   "source": [
    "# Check the first batch from the DataLoader\n",
    "images, _ = next(iter(data_loader))\n",
    "class_names = dataset.classes\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# Print class names\n",
    "print(f'Classes: {class_names}')\n",
    "\n",
    "# Print shape details\n",
    "print(f\"Batch Size: {images.shape[0]}\")\n",
    "print(f\"Channels: {images.shape[1]}\")\n",
    "print(f\"Height of Image: {images.shape[2]}\")\n",
    "print(f\"Width of Image: {images.shape[3]}\")  \n",
    "\n",
    "# Count total images in dataset\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "print(f\"Total Images: {dataset_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404f18f",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0b6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mean and std based on pretrained weights of pretrained model\n",
    "# ImageNet Statistics\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422b9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to resize! shape is already 224 x 224 px\n",
    "train_transform = transforms.Compose([\n",
    "    # convert images to pytorch tensors\n",
    "    transforms.ToTensor(), \n",
    "    # randomly flips images for augmentation\n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    # normalize images based on ImageNet mean and std\n",
    "    transforms.Normalize(mean, std) \n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=training_dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11be1840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827a0b2",
   "metadata": {},
   "source": [
    "# Setup Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19f09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4 # healthy, cordona, pestalotiopsis, sigatoka\n",
    "swin_model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes = 4, global_pool='avg')\n",
    "swin_model = swin_model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD = Stochastic Gradient Descent\n",
    "# lr = learning rate (test values from [0.001, 0.01] or experiment others)\n",
    "optimizer = optim.SGD(\n",
    "    swin_model.parameters(), \n",
    "    lr=0.001, \n",
    "    momentum=0.9, \n",
    ")\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "writer = SummaryWriter('VIT_transfer_learning/runs/swin_tiny')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a00ddc",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ab79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, scheduler, device, criterion, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar(\"Batch Loss\", loss.item(), global_step=step)\n",
    "            acc = (predicted == labels).sum().item() / labels.size(0)\n",
    "            writer.add_scalar(\"Batch Accuracy\", acc, global_step=step)\n",
    "            step += 1\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += (predicted == labels).sum().item()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_accuracy = 100 * running_corrects / dataset_size\n",
    "        writer.add_scalar(\"Epoch Training Loss\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Epoch Training Accuracy\", epoch_accuracy, epoch)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f\"Weights/{name}\", param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"Gradients/{name}\", param.grad, epoch)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "        if epoch_accuracy > best_acc:\n",
    "            best_acc = epoch_accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be69523",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d1a6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phea\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\PIL\\TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "C:\\Users\\phea\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\PIL\\JpegImagePlugin.py:890: UserWarning: Image appears to be a malformed MPO file, it will be interpreted as a base JPEG file\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 0.9224, Train Accuracy: 60.26%\n",
      "Epoch [2/30], Train Loss: 0.2376, Train Accuracy: 95.02%\n",
      "Epoch [3/30], Train Loss: 0.0688, Train Accuracy: 98.58%\n",
      "Epoch [4/30], Train Loss: 0.0289, Train Accuracy: 99.53%\n",
      "Epoch [5/30], Train Loss: 0.0153, Train Accuracy: 100.00%\n",
      "Epoch [6/30], Train Loss: 0.0164, Train Accuracy: 99.88%\n",
      "Epoch [7/30], Train Loss: 0.0118, Train Accuracy: 99.88%\n",
      "Epoch [8/30], Train Loss: 0.0082, Train Accuracy: 100.00%\n",
      "Epoch [9/30], Train Loss: 0.0066, Train Accuracy: 99.88%\n",
      "Epoch [10/30], Train Loss: 0.0055, Train Accuracy: 99.88%\n",
      "Epoch [11/30], Train Loss: 0.0052, Train Accuracy: 100.00%\n",
      "Epoch [12/30], Train Loss: 0.0029, Train Accuracy: 100.00%\n",
      "Epoch [13/30], Train Loss: 0.0053, Train Accuracy: 99.88%\n",
      "Epoch [14/30], Train Loss: 0.0041, Train Accuracy: 100.00%\n",
      "Epoch [15/30], Train Loss: 0.0029, Train Accuracy: 100.00%\n",
      "Epoch [16/30], Train Loss: 0.0053, Train Accuracy: 100.00%\n",
      "Epoch [17/30], Train Loss: 0.0032, Train Accuracy: 100.00%\n",
      "Epoch [18/30], Train Loss: 0.0039, Train Accuracy: 100.00%\n",
      "Epoch [19/30], Train Loss: 0.0032, Train Accuracy: 100.00%\n",
      "Epoch [20/30], Train Loss: 0.0043, Train Accuracy: 99.88%\n",
      "Epoch [21/30], Train Loss: 0.0038, Train Accuracy: 100.00%\n",
      "Epoch [22/30], Train Loss: 0.0033, Train Accuracy: 100.00%\n",
      "Epoch [23/30], Train Loss: 0.0029, Train Accuracy: 100.00%\n",
      "Epoch [24/30], Train Loss: 0.0038, Train Accuracy: 100.00%\n",
      "Epoch [25/30], Train Loss: 0.0036, Train Accuracy: 100.00%\n",
      "Epoch [26/30], Train Loss: 0.0036, Train Accuracy: 100.00%\n",
      "Epoch [27/30], Train Loss: 0.0039, Train Accuracy: 100.00%\n",
      "Epoch [28/30], Train Loss: 0.0030, Train Accuracy: 100.00%\n",
      "Epoch [29/30], Train Loss: 0.0030, Train Accuracy: 100.00%\n",
      "Epoch [30/30], Train Loss: 0.0034, Train Accuracy: 100.00%\n",
      "Training complete in 76m 43s\n"
     ]
    }
   ],
   "source": [
    "best_model = train_model(\n",
    "    swin_model, \n",
    "    train_loader, \n",
    "    step_lr_scheduler, \n",
    "    device, \n",
    "    loss_function, \n",
    "    optimizer, \n",
    "    num_epochs\n",
    ")\n",
    "\n",
    "# Change path to new model save file\n",
    "swin_trained_model = \"VIT_transfer_learning/models/swin_tiny_1.pth\"\n",
    "torch.save(best_model.state_dict(), swin_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6674daa",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e233abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, criterion):\n",
    "    since = time.time()\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += (labels == predicted).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / test_dataset_size\n",
    "    epoch_accuracy = 100 * running_corrects / test_dataset_size\n",
    "\n",
    "    print(f\"Eval Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    print(f'Got {running_corrects} out of {test_dataset_size} images correctly')\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Evaluation complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4f630",
   "metadata": {},
   "source": [
    "# Practice Test Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b0a4c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phea\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\PIL\\TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "C:\\Users\\phea\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\PIL\\JpegImagePlugin.py:890: UserWarning: Image appears to be a malformed MPO file, it will be interpreted as a base JPEG file\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.0300, Accuracy: 99.33%\n",
      "Got 2520 out of 2537 images correctly\n",
      "Evaluation complete in 3m 51s\n"
     ]
    }
   ],
   "source": [
    "test_dataset_path = './prac_test/'\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=test_dataset_path, \n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(  \n",
    "    dataset=test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "num_classes = 4 # healthy, cordona, pestalotiopsis, sigatoka\n",
    "swin_model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes = 4, global_pool='avg')\n",
    "swin_model_trained = \"VIT_transfer_learning/models/swin_tiny_1.pth\"\n",
    "state_dict = torch.load(swin_model_trained)\n",
    "swin_model.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "test_dataset_size = len(test_dataset)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss, accuracy = evaluate_model(swin_model, test_loader, device, loss_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501f892",
   "metadata": {},
   "source": [
    "# Run TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d2bf7",
   "metadata": {},
   "source": [
    "Download models and runs here https://drive.google.com/drive/folders/1vO8MprVQdztSgorqvVyTRAQZL_XACgEh?usp=drive_link\n",
    "\n",
    "Run in terminal\n",
    "1. `tensorboard-env\\Scripts\\activate`\n",
    "2. `tensorboard --logdir=CNN_transfer_learning\\runs` If it doesnt work, install first tensorboard within env\n",
    "3. Open at http://localhost:6006/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
