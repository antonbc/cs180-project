{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4f3f5-52ea-4d9d-a3e0-1429f0d78be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c976d1-ed1e-4a7a-a584-850fbfccb2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c7519-fce1-4113-b392-8b064d1888d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset path\n",
    "training_dataset_path = './train/'\n",
    "\n",
    "# path to each dataset\n",
    "healthy_banana_dataset_path = './train/healthy/'\n",
    "cordona_banana_dataset_path = './train/cordana/'\n",
    "pestalotiopsis_banana_dataset_path = './train/pestalotiopsis/'\n",
    "sigatoka_banana_dataset_path = './train/sigatoka/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "029dc14a-2d15-4b40-9c18-a7490c3f0a32",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608527f9-2538-4fdc-8fdb-63c0cb69be30",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f989b91-a45a-4df3-84a3-9935f4ed14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=training_dataset_path, \n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606eea1-e84b-40a5-8a3e-96cf71a2e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first batch from the DataLoader\n",
    "images, _ = next(iter(data_loader))\n",
    "class_names = dataset.classes\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# Print class names\n",
    "print(f'Classes: {class_names}')\n",
    "\n",
    "# Print shape details\n",
    "print(f\"Batch Size: {images.shape[0]}\")\n",
    "print(f\"Channels: {images.shape[1]}\")\n",
    "print(f\"Height of Image: {images.shape[2]}\")\n",
    "print(f\"Width of Image: {images.shape[3]}\")  \n",
    "\n",
    "# Count total images in dataset\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "print(f\"Total Images: {dataset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0250d-a319-4006-9b40-ffc57ccfdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataset(dataset):\n",
    "    # batch_size = number of images shown\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=5, shuffle=True) \n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images, nrow=5)\n",
    "    title = [class_names[x] for x in labels]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(title)\n",
    "    plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0773cb",
   "metadata": {},
   "source": [
    "# Show Dataset without Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878edddb-5bb0-42fb-8fb4-13d8feaadaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataset with no transformation\n",
    "train_dataset_no_transform = torchvision.datasets.ImageFolder(\n",
    "    root=training_dataset_path, \n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "show_dataset(train_dataset_no_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4361898e",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mean and std based on pretrained weights of pretrained model\n",
    "# ImageNet Statistics\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59bd1f-b572-453a-874d-6ca27165caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to resize! shape is already 224 x 224 px\n",
    "train_transform = transforms.Compose([\n",
    "    # convert images to pytorch tensors\n",
    "    transforms.ToTensor(), \n",
    "    # randomly flips images for augmentation\n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    # normalize images based on ImageNet mean and std\n",
    "    transforms.Normalize(mean, std) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195aceb",
   "metadata": {},
   "source": [
    "# Show Dataset with Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ef729-5c73-44cb-a8e6-429d9a0beb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataset with transformations\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=training_dataset_path, transform=train_transform)\n",
    "show_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16e42a-9ddb-4626-9047-8c5d522e4809",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f77176-1405-4df0-8efa-76fb16950895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = transformed and final training dataset \n",
    "# change batch_size depending on the performance of the model\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca350a30-986a-4b5c-a57d-9fcc7f2dc0d2",
   "metadata": {},
   "source": [
    "## Setup Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8ec6b-f172-4e0b-b6ad-0f02401f6beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152_model = models.resnet152(weights='DEFAULT')\n",
    "# https://pytorch.org/vision/0.17/models/generated/torchvision.models.resnet152.html#torchvision.models.ResNet152_Weights\n",
    "\n",
    "num_features = resnet152_model.fc.in_features\n",
    "num_classes = 4 # healthy, cordona, pestalotiopsis, sigatoka\n",
    "resnet152_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Move model to device\n",
    "resnet152_model = resnet152_model.to(device)\n",
    "\n",
    "# https://pytorch.org/docs/stable/nn.html#loss-functions \n",
    "# try other options\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD = Stochastic Gradient Descent\n",
    "# lr = learning rate (test values from [0.001, 0.01] or experiment others)\n",
    "optimizer = optim.SGD(\n",
    "    resnet152_model.parameters(), \n",
    "    lr=0.001, \n",
    "    momentum=0.9, \n",
    "    weight_decay=0.003\n",
    ")\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887aaa8-1459-4078-af38-34f724ce5414",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3cdb0-2883-4501-a0ce-398452beb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, scheduler, device, criterion, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += (predicted == labels).sum().item()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_accuracy = 100 * running_corrects / dataset_size\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "        if epoch_accuracy > best_acc:\n",
    "            best_acc = epoch_accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278456ac",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ee5c2-88e2-4293-b27f-554392343f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = train_model(\n",
    "    resnet152_model, \n",
    "    train_loader, \n",
    "    step_lr_scheduler, \n",
    "    device, \n",
    "    loss_function, \n",
    "    optimizer, \n",
    "    20\n",
    ")\n",
    "\n",
    "resnet152_trained_model = \"resnet152_trained_model.pth\"\n",
    "torch.save(best_model.state_dict(), resnet152_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288292d-53ca-4a0b-a363-d78a832b0aec",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7005d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += (labels == predicted).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    epoch_accuracy = 100 * running_corrects / dataset_size\n",
    "\n",
    "    print(f\"Eval Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    print(f'Got {running_corrects} out of {dataset_size} images correctly')\n",
    "    print('Eval finished')\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8b123",
   "metadata": {},
   "source": [
    "# Practice Test Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset from https://www.kaggle.com/datasets/shifatearman/bananalsd?resource=download\n",
    "\n",
    "# # Original + Augmented\n",
    "# test_dataset_path = './prac_test/'\n",
    "\n",
    "# mean = np.array([0.485, 0.456, 0.406])\n",
    "# std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean, std)\n",
    "# ]),\n",
    "\n",
    "# test_dataset = torchvision.datasets.ImageFolder(\n",
    "#     root=test_dataset_path, \n",
    "#     transform=test_transform\n",
    "# )\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(  \n",
    "#     dataset=test_dataset, \n",
    "#     batch_size=32, \n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# # Define the model architecture\n",
    "# resnet152_model = models.resnet152(weights='DEFAULT')\n",
    "# num_features = resnet152_model.fc.in_features\n",
    "# num_classes = 4\n",
    "# resnet152_model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# # Load the trained weights\n",
    "# resnet152_trained_model = \"resnet152_trained_model.pth\"\n",
    "# state_dict = torch.load(resnet152_trained_model) \n",
    "# resnet152_model.load_state_dict(state_dict)\n",
    "\n",
    "# # Evaluate model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using {device} device\")\n",
    "# loss, accuracy = evaluate_model(resnet152_model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
